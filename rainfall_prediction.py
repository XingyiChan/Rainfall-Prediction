# -*- coding: utf-8 -*-
"""rainfall_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fyOgMPiOmjV_1L823ZAEcPzfDGh2NMO7

###Data preprocessing steps: 
- Dealing with outliers (not required)

- Remove unwanted columns
    - removed Risk_MM, Location and Data column

- Dealing with missing values
    - replacing MinTemp and MaxTemp columns with mean values
    - replacing Rainfall and RainToday columns with most frequent values
    - replacing the rest NaN values to zeros

- Feature encoding (encoding categorical features)
    - for rainToday and rainTomorrow column
        - map {'Yes': 1, 'No': 0}
    - for WindGustDir, WindDir9am and WindDir3pm columns
        - map {'N':1, 'NNE':2, 'NE':3, 'ENE':4, 'E':5, 'NSE':6, 'SE':7, 'SSE':8 'S':9, 'SSW':10, 'SW':11, 'WSW':12, 'W':13, 'WNW':14, 'NW':15, 'NNW':16}

- Data Scaling
    - Standardize values in columns Pressure9am and Pressure3pm
        - values will be standardize to standard deviation of 0 to 1

- Handling Imbalance
    - Data is split into 70% training data and 30% test data
    - SMOTE method is used to rebalance the data
        - is an oversampling technique that allows us to creates new artificial data points in our dataset 

- Feature Selection
    - Use random forest to find the importance of each features and select the top 12 most important features
        - The selected features are WindSpeed3pm, Cloud3pm, Temp9am, MaxTemp, MinTemp, Temp3pm, Pressure9am, Humidity9am, WindGustSpeed, Pressure3pm, Rainfall, Humidity3pm
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import f_regression

file_path = '/content/drive/My Drive/Msc_AI Research/weatherAUS.csv'

'''read data into dataframe'''
oridata = pd.read_csv(file_path) #original dataset
data = pd.read_csv(file_path) #edited dataset
# just take a subset of the data
#data = data.iloc[:50000, :]

#set rows and columns for display
pd.set_option('display.max_rows', 50)
pd.set_option('display.max_columns', 30)

#============================================================
#                      drop unwated columns
#============================================================

'''drop RISK_MM column'''
data = data.drop(['RISK_MM'], axis = 1)

'''drop Location column'''
data = data.drop(['Location'], axis = 1)

'''drop Date column'''
data = data.drop(['Date'], axis = 1)

#============================================================
#                  dealing with missing values
#============================================================

'''replace missing values with mean values'''
imputerMean = SimpleImputer(missing_values = np.NaN, strategy = 'mean')

'''replace missing values for MinTemp column'''
imputerMean.fit(data[['MinTemp']])
data['MinTemp'] = imputerMean.transform(data[['MinTemp']])
'''replace missing values for MaxTemp column'''
imputerMean.fit(data[['MaxTemp']])
data['MaxTemp'] = imputerMean.transform(data[['MaxTemp']])


'''replace missing values with most frequent values'''
imputerMF = SimpleImputer(missing_values = np.NaN, strategy = 'most_frequent')

'''replace missing values for Rainfall column'''
imputerMF.fit(data[['Rainfall']])
data['Rainfall'] = imputerMF.transform(data[['Rainfall']])
'''replace missing values for RainToday column'''
imputerMF.fit(data[['RainToday']])
data['RainToday'] = imputerMF.transform(data[['RainToday']])

#============================================================
#                   data categorization
#============================================================
'''categorize data for RainToday and RainTomorrow column'''
'''using mapping function'''
rain_mapping = {'Yes': 1, 'No': 0}
data['RainToday'] = data['RainToday'].map(rain_mapping)
data['RainTomorrow'] = data['RainTomorrow'].map(rain_mapping)

'''categorize data for WindGustDir,  WindDir9am and WindDir3pm column'''
dir_mapping = {'N':1, 'NNE':2, 'NE':3, 'ENE':4, 'E':5, 'NSE':6, 'SE':7, 'SSE':8, 
               'S':9, 'SSW':10, 'SW':11, 'WSW':12, 'W':13, 'WNW':14, 'NW':15, 'NNW':16}
data['WindGustDir'] = data['WindGustDir'].map(dir_mapping)
data['WindDir9am'] = data['WindDir9am'].map(dir_mapping)
data['WindDir3pm'] = data['WindDir3pm'].map(dir_mapping)

'''replace the rest with zeros'''
#data = data.replace(np.NaN, 0)
data = data.dropna()

#============================================================
#                   data scaling
#============================================================
'''scaling data'''
scaler = preprocessing.StandardScaler() #standardization (standard deviation of 1)
scaler2 = preprocessing.MinMaxScaler() #normalization (range between 0 n 1)

'''scaling data using standardization method'''
data[['Pressure9am', 'Pressure3pm']] = scaler.fit_transform( data[['Pressure9am', 'Pressure3pm']])

#============================================================
#                   data rebalancing
#============================================================
'''split data before doing data rebalancing'''

'''seperate target value and features'''
Label = data.iloc[:, -1]
Feature = data.iloc[:, :-1]

# '''data is split into 30% test data and 70% train data'''
trainFeature, testFeatures, trainLabel, testLabel = train_test_split(Feature, Label, test_size = 0.3, random_state = 40)

'''use SMOTE to rebalance data'''
sm = SMOTE(random_state = 0)

trainFeature, trainLabel = sm.fit_sample(trainFeature, trainLabel)

trainFeatures = pd.DataFrame({'MinTemp': trainFeature[:, 0], 'MaxTemp': trainFeature[:, 1], 'Evaporation': trainFeature[:, 2], 'Rainfall': trainFeature[:, 3], 'Sunshine': trainFeature[:, 4], 
                             'WindGustDir': trainFeature[:, 5], 'WindGustSpeed': trainFeature[:, 6], 'WindDir9am': trainFeature[:, 7], 'WindDir3pm': trainFeature[:, 8], 'WindSpeed9am': trainFeature[:, 9],
                             'WindSpeed3pm': trainFeature[:, 10], 'Humidity9am': trainFeature[:, 11], 'Humidity3pm': trainFeature[:, 12], 'Pressure9am': trainFeature[:, 13], 'Pressure3pm': trainFeature[:, 14],
                             'Cloud9am': trainFeature[:, 15], 'Cloud3pm': trainFeature[:, 16], 'Temp9am': trainFeature[:, 17], 'Temp3pm': trainFeature[:, 18], 'RainToday': trainFeature[:, 19]})


#============================================================
#                   feature selection
#============================================================
def randomForest():
    forest = RandomForestClassifier(n_estimators = 100, criterion='entropy', random_state = 0)
    forest.fit(Feature, Label)
    importances = forest.feature_importances_
    #plot graph of feature importances for better visualization
    feat_importances = pd.Series(importances, index=Feature.columns)
    feat_importances.nlargest(12).plot(kind='barh')
    plt.show()
#randomForest()

def univariate():
    selector = SelectPercentile(f_regression, percentile=25)
    selector.fit(Feature, Label)
    importances = selector.scores_
    #plot graph of feature importances for better visualization
    feat_importances = pd.Series(importances, index=Feature.columns)
    feat_importances.nlargest(12).plot(kind='barh')
    plt.show()
#univariate()

'''selecting top 12 most important features'''
# Univariate
#data = data[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]
#trainFeatures = trainFeatures[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]
#testFeatures = testFeatures[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]

# Random forest
# data = data[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]
# trainFeatures = trainFeatures[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]
# testFeatures = testFeatures[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]

#export processed data to excel file
#trainFeatures.to_csv('data_output.csv', encoding='utf-8', index=False)

#============================================================
#                           SVM Model
#============================================================
def SVM():
    clf = SVC(kernel='poly', C=1.0,  degree=10, gamma='scale')
    accuracy = cross_val_score(clf, trainFeatures, trainLabel, cv = 10)
    print ("Support Vector Machine score: ", accuracy.mean())
    #clf.fit(trainFeatures, trainLabel)
    #accuracy = (clf.score(testFeatures, testLabel))* 100
    #print()
    #print ('Accuracy is: ', accuracy) 

#============================================================
#                       Random Forest Model
#============================================================
def RF():
    clf = RandomForestClassifier()
    accuracy = cross_val_score(clf, trainFeatures, trainLabel, cv = 10)
    print ("Random Forest score: ", accuracy.mean())

#============================================================
#                  Gaussian Naive Bayes Model
#============================================================
def GNB():
    clf = GaussianNB()
    accuracy = cross_val_score(clf, trainFeatures, trainLabel, cv = 10)
    print ("Naive Bayes score: ", accuracy.mean())

#============================================================
#                  Logistic Regression Model
#============================================================
def LogisReg():
    clf = LogisticRegression()
    accuracy = cross_val_score(clf, trainFeatures, trainLabel, cv = 10)
    print ("Naive Bayes score: ", accuracy.mean())