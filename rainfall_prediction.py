# -*- coding: utf-8 -*-
"""rainfall_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fyOgMPiOmjV_1L823ZAEcPzfDGh2NMO7
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import f_regression

file_path = '/content/drive/My Drive/Msc_AI Research/weatherAUS.csv'

'''read data into dataframe'''
data = pd.read_csv(file_path)

#============================================================
#                      drop unwated columns
#============================================================

'''drop RISK_MM column'''
data = data.drop(['RISK_MM'], axis = 1)

'''drop Location column'''
data = data.drop(['Location'], axis = 1)

'''drop Date column'''
data = data.drop(['Date'], axis = 1)

#============================================================
#                  dealing with missing values
#============================================================

'''replace missing values with mean values'''
imputerMean = SimpleImputer(missing_values = np.NaN, strategy = 'mean')

'''replace missing values for MinTemp column'''
imputerMean.fit(data[['MinTemp']])
data['MinTemp'] = imputerMean.transform(data[['MinTemp']])
'''replace missing values for MaxTemp column'''
imputerMean.fit(data[['MaxTemp']])
data['MaxTemp'] = imputerMean.transform(data[['MaxTemp']])


'''replace missing values with most frequent values'''
imputerMF = SimpleImputer(missing_values = np.NaN, strategy = 'most_frequent')

'''replace missing values for Rainfall column'''
imputerMF.fit(data[['Rainfall']])
data['Rainfall'] = imputerMF.transform(data[['Rainfall']])
'''replace missing values for RainToday column'''
imputerMF.fit(data[['RainToday']])
data['RainToday'] = imputerMF.transform(data[['RainToday']])

#============================================================
#                   data categorization
#============================================================
'''categorize data for RainToday and RainTomorrow column'''
'''using mapping function'''
rain_mapping = {'Yes': 1, 'No': 0}
data['RainToday'] = data['RainToday'].map(rain_mapping)
data['RainTomorrow'] = data['RainTomorrow'].map(rain_mapping)

'''categorize data for WindGustDir,  WindDir9am and WindDir3pm column'''
dir_mapping = {'N':1, 'NNE':2, 'NE':3, 'ENE':4, 'E':5, 'NSE':6, 'SE':7, 'SSE':8, 
               'S':9, 'SSW':10, 'SW':11, 'WSW':12, 'W':13, 'WNW':14, 'NW':15, 'NNW':16}
data['WindGustDir'] = data['WindGustDir'].map(dir_mapping)
data['WindDir9am'] = data['WindDir9am'].map(dir_mapping)
data['WindDir3pm'] = data['WindDir3pm'].map(dir_mapping)

#============================================================
#                   data scaling
#============================================================
'''scaling data'''
scaler = preprocessing.StandardScaler() #standardization (mean of 0 and standard deviation of 1)

'''scaling data using standardization method'''
data[['Pressure9am', 'Pressure3pm']] = scaler.fit_transform( data[['Pressure9am', 'Pressure3pm']])

#============================================================
#                   data rebalancing
#============================================================
'''split data before doing data rebalancing'''

'''seperate labels and features'''
Label = data.iloc[:, -1]
Feature = data.iloc[:, :-1]

# '''data is split into 30% test data and 70% train data'''
trainFeature, testFeatures, trainLabel, testLabel = train_test_split(Feature, Label, test_size = 0.3, random_state = 40)

'''use SMOTE to rebalance data'''
sm = SMOTE(random_state = 0)

trainFeature, trainLabel = sm.fit_sample(trainFeature, trainLabel)

#============================================================
#                   feature selection
#============================================================

def univariate():
    selector = SelectPercentile(f_regression, percentile=25)
    selector.fit(Feature, Label)
    importances = selector.scores_
    #plot graph of feature importances for better visualization
    feat_importances = pd.Series(importances, index=Feature.columns)
    feat_importances.nlargest(12).plot(kind='barh')
    plt.show()
univariate()

'''selecting top 12 most important features'''
# Univariate
#data = data[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]
#trainFeatures = trainFeatures[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]
#testFeatures = testFeatures[['Pressure9am', 'Pressure3pm', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'Rainfall', 'Humidity9am', 'Cloud9am', 'Cloud3pm', 'RainToday', 'Humidity3pm']]

# Random forest
data = data[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]
trainFeatures = trainFeatures[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]
testFeatures = testFeatures[['Temp9am', 'Pressure9am', 'MaxTemp', 'WindGustSpeed', 'Temp3pm', 'Sunshine', 'MinTemp', 'Rainfall', 'Humidity9am', 'Cloud3pm', 'Pressure3pm', 'Humidity3pm']]

#export processed data to excel file
#trainFeatures.to_csv('data_output.csv', encoding='utf-8', index=False)

#============================================================
#                           SVM Model
#============================================================
def SVM():
    clf = SVC(kernel='poly', C=1.0,  degree=10, gamma='scale')
    clf.fit(trainFeatures, trainLabel)
    accuracy = (clf.score(testFeatures, testLabel))* 100
    print()
    print ('Accuracy is:', accuracy) 
SVM()

#============================================================
#                  Gaussian Naive Bayes Model
#============================================================
def GNB():
    clf = GaussianNB()
    clf.fit(trainFeatures, trainLabel)
    accuracy = (clf.score(testFeatures, testLabel))* 100
    print()
    print ('Accuracy is: ', accuracy) 
GNB()

#============================================================
#                  Logistic Regression Model
#============================================================
def LogisReg():
    clf = LogisticRegression()
    clf.fit(trainFeatures, trainLabel)
    accuracy = (clf.score(testFeatures, testLabel))* 100
    print()
    print ('Accuracy is: ', accuracy) 
LogisReg()

#============================================================
#                    Random Forest Model
#============================================================
def RF():
    clf = RandomForestClassifier()
    clf.fit(trainFeatures, trainLabel)
    accuracy = (clf.score(testFeatures, testLabel))* 100
    print()
    print ('Accuracy is : ', accuracy) 
RF()

#============================================================
#              Random Forest for Feature Selection
#============================================================
def randomForest():
    forest = RandomForestClassifier(n_estimators = 100, criterion='entropy', random_state = 0)
    forest.fit(Feature, Label)
    importances = forest.feature_importances_
    #plot graph of feature importances for better visualization
    feat_importances = pd.Series(importances, index=Feature.columns)
    feat_importances.nlargest(12).plot(kind='barh')
    plt.show()
randomForest()

#============================================================
#                           SVM Model
#============================================================
def SVM():
    clf = SVC(kernel='poly', C=1.0,  degree=10, gamma='scale')
    clf.fit(trainFeatures, trainLabel)
    accuracy = (clf.score(testFeatures, testLabel))* 100
    print()
    print ('Accuracy is: ', accuracy) 
SVM()